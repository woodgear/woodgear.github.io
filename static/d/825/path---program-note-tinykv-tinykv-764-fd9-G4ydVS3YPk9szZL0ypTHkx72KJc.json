{"data":{"markdownRemark":{"html":"<h1>Raft</h1>\n<p>Raft实际上要求使用者是必须是状态机模型,Raft负责保证状态机的输入是一致的,\n在此之上,我们就可以通过在启动多个同样的状态机,来保证当出现状态机本身逻辑之外的错误时,能有一个未遭遇此错误的状态机正常,从而达到容错的作用\n每个Node维护一个command列表,当确定集群中大多数节点共享相同的commands时(达成共识),将这些command应用到应用的状态机中,达成共识的commandlist是不可修改的(imuate)</p>\n<p>既然我们的应用是状态机模型.达成共识的一个最简单的方法是,所有的请求都只给一个节点(将其称之为Leader节点),这个节点收到请求后,将请求存储起来(维护自己的command列表), 等到确认所有其他的Node(将其称之为follower)的commandlist和自己的commandlist一致后将这个command喂给应用,所谓的确保所有Node一致,最简单的做法就是向所有follower发请求,强行将自己的commandlist灌进去.,这样的话在每个节点(follower)上可以保证他们存储的commandlist与被选中的那个节点(leader)是一致的(达成共识的).在这种情况下,实际上无论谁被选中都是一样的,Node的状态是一致的,他们是等价的.现在,假设这个被选中的Node(leader)突然出现了问题(非逻辑问题,而是诸如断网,磁盘空间不够等问题),我们可以直接请求其他的Node,因为他们是等价的,这样就是所谓的容错性.</p>\n<p>Raft描述的就是在 </p>\n<ol>\n<li>上述概念下的优化(实际上不用将全部的cmdlist发给follower),</li>\n<li>一些具体的关于如何维护节点commandlist一致,和为什么这样就一致了的证明,</li>\n<li>如何选择Leader节点,</li>\n<li>与在此之上的一些操作(扩容),</li>\n<li>和从现实角度考虑会遇到的一些问题的解决方法(commandlist是无限的,但存储是有限的 snapshot)</li>\n<li>客户端如何向一个Raft集群发起请求</li>\n</ol>\n<h2>tiny</h2>\n<ol>\n<li>TinyKV 将Raft抽象封装到Node这个概念中,使用Node的人,通过读取Node.Ready()方法返回的command list来获取那些达成共识的command.将这些command应用到状态机中</li>\n<li>由使用者维护HardSate(Config信息) Entries(命令) Snapshot(使用者自身状态) 的持久化</li>\n<li>协助Node之间完成交流,将Node的command发送给对应的Node,将接受到的command交给自己的Node</li>\n<li>定时调用Node的tick方法,来保证Node内部一些依赖于时间的操作(heartbeattimeout,electionTimeout) 正常完成</li>\n</ol>\n<h3>重启</h3>\n<p>在重启时,Raft Node 需要能够帮助状态机恢复状态</p>\n<ol>\n<li>对于保存起来的commandlist 和snapshot 这个snapshot的最后一个command的Index</li>\n</ol>\n<h3>snapshot</h3>\n<p>对于一个内存状态机(状态机所有状态维护在内存中)来讲,如果想要恢复到其中的某个状态 1. 从头开始将commands再执行一遍 2. 想办法将状态机的内存表示持久化起来<br>\nsnapshot实际上就是内存状态机的持久话的内存表示方式.<br>\n有了snapshot之后,达到这个状态之前的log实际上已经不需要了 可以直接删除.</p>\n<h3>ready/Advance 指的是什么?</h3>\n<p>Ready: 有新的index被commit了\nAdvance: 应用已经应用了Commit的Index,可以apply了</p>\n<h2>log replication</h2>\n<blockquote>\n<p>最简单的做法就是向所有follower发请求,强行将自己的commandlist灌进去</p>\n</blockquote>\n<p>最终要达成的效果是所有Raft节点上的log一致,但方法肯定不是上面那种简单粗暴的方式.\nRaft共识实际上是一个链式推论的过程, 1. preLogIndex和preLogTerm能够匹配上 + leader只会发送连续的以preLogIndex为前缀的entries,就可已的出所有的log都是连续的\n如果leader知道ofollower的log和自己相同的那部分(preLogIndex),那么实际上只要发送preLogIndex之后的部分即可了.\n问题在于如何知道\n一个简单的方法就是从leader的lastIndex开始发起,一直倒退 总有一个是能够与这个follower匹配上的.</p>\n<p>还有一个问题是leader需要知道follower的lastIndex,这样leader才能确信log已经被分发到了大多数的节点上才能commit log\n当leader成功的将自己的log分发给follower之后,leader可以确信follower的lastindex和自己的lastindex是一致的.</p>\n<p>所以leader需要知道follower的log和自己的相同的部分 </p>\n<p>所以为了维护这种信息 leader最起码需要为每个follower维护两个值 Match,Next.\nMatch: leader确信的follower和自己一定一样的Index,\nNext: leader实际上可以直接将Match->LastIndex发给follower,但leader一开始是不知道follower的Match是多少的,Leader必须从LastIndex倒退的尝试去寻找Match,所以需要Next来维护这个游标 </p>\n<p>因此整个流程是这样的</p>\n<ol>\n<li>leader初始化 设置Match为0,Next为LastIndex</li>\n<li>使用Next-1为PreLoIndex 向follower发送Append\n3-1. 成功 说明follower的log已经与leader的一致 我们可以得知最新的Match的信息,设置Next为Match+1,可以开始尝试commit了 TODO ??? 发送与返回异步的问题\n3-2. 失败 说明PreLogIndex选取的不对,将Next递减 回到步骤2</li>\n</ol>\n<h1>tips</h1>\n<p>candidat选举超时立刻开始新一轮的选举</p>\n<h1>question</h1>\n<h2>假设一个raft节点 网络突然隔离 其会一直递增自己的选期,突然网络好了,会导致整个raftu集群被重置?</h2>\n<h2>一个raftnode在重启后能重置那些信息?</h2>\n<p>lastIndex,lastIndexTerm 可以直接从logs中拿到</p>\n<h2>pers 的matchIndex和nextIndex是如何工作的</h2>\n<p>match初始化为0 next初始化为lastIndex+1,在sendAppend时preLogIndex实际上是next-1 entries是[next,lastIndex],在AppendResponse时更新Next为LastIndex+1 Match=Next</p>\n<h2>如何保证leader不会commit之前的term的Index?</h2>\n<h2>当pers是偶数时多数如何计算?</h2>\n<h2>在初始化状态时 Prs的值是什么 AppendResponse 如何导致Prs的变化 sendApeend时如何根据Prs发送?</h2>"}},"pageContext":{"isCreatedByStatefulCreatePages":false,"slug":"/program-note/tinykv/tinykv/"}}